{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Helpful tutorial links \u00b6 https://snakemake.readthedocs.io/en/stable/tutorial/basics.html https://snakemake.readthedocs.io/en/stable/tutorial/advanced.html https://carpentries-incubator.github.io/workflows-snakemake/","title":"Background"},{"location":"#helpful-tutorial-links","text":"https://snakemake.readthedocs.io/en/stable/tutorial/basics.html https://snakemake.readthedocs.io/en/stable/tutorial/advanced.html https://carpentries-incubator.github.io/workflows-snakemake/","title":"Helpful tutorial links"},{"location":"usage/example/","text":"Dry Run Expected Output \u00b6 The output of the dry-run for Rule A should look as follows: job count min threads max threads ----- ------- ------------- ------------- A 2 1 1 all 1 1 1 total 3 1 1 The output of the dry-run for Rules A-B should look as follows: job count min threads max threads ----- ------- ------------- ------------- A 2 1 1 B 2 1 1 all 1 1 1 total 5 1 1 The output of the dry-run for Rules A-C should look as follows: job count min threads max threads ----- ------- ------------- ------------- A 2 1 1 B 2 1 1 C 1 1 1 all 1 1 1 total 6 1 1 The output of the dry-run for Rules A-D should look as follows: job count min threads max threads ----- ------- ------------- ------------- A 2 1 1 B 2 1 1 C 1 1 1 D 2 1 1 all 1 1 1 total 8 1 1 The output of the dry-run for Rules A-E should look as follows: job count min threads max threads ----- ------- ------------- ------------- A 2 1 1 B 2 1 1 C 1 1 1 D 2 1 1 E 2 1 1 all 1 1 1 total 10 1 1 Expected Output Files / Structure \u00b6 The output of Rule A: \u251c\u2500\u2500 sample_1_rulea.txt \u251c\u2500\u2500 sample_2_rulea.txt The output of Rule B: \u251c\u2500\u2500 sample_1_rulea.txt \u251c\u2500\u2500 sample_1_ruleb.txt \u251c\u2500\u2500 sample_2_rulea.txt \u251c\u2500\u2500 sample_2_ruleb.txt The output of Rule C: \u251c\u2500\u2500 final_output \u2502 \u251c\u2500\u2500 merged_rulea.txt \u251c\u2500\u2500 sample_1_rulea.txt \u251c\u2500\u2500 sample_1_ruleb.txt \u251c\u2500\u2500 sample_2_rulea.txt \u2514\u2500\u2500 sample_2_ruleb.txt The output of Rule D: \u251c\u2500\u2500 final_output \u2502 \u251c\u2500\u2500 merged_rulea.txt \u2502 \u251c\u2500\u2500 sample_1_copied_ruleb.txt \u2502 \u251c\u2500\u2500 sample_2_copied_ruleb.txt \u251c\u2500\u2500 sample_1_rulea.txt \u251c\u2500\u2500 sample_1_ruleb.txt \u251c\u2500\u2500 sample_2_rulea.txt \u2514\u2500\u2500 sample_2_ruleb.txt The output of advanced tasks: final_output/ \u251c\u2500\u2500 merged_rulea.txt \u251c\u2500\u2500 sample_1_copied_ruleb.txt \u2514\u2500\u2500 sample_2_copied_ruleb.txt The output of Rule E: final_output/ \u251c\u2500\u2500 merged_rulea.txt \u251c\u2500\u2500 sample_1_copied_ruleb.txt \u251c\u2500\u2500 sample_1.sam \u251c\u2500\u2500 sample_2_copied_ruleb.txt \u2514\u2500\u2500 sample_2.sam Explanations \u00b6 General Notes \u00b6 The join and expand functions the join function is used to create and manage path structure by converting all \",\" to the expand expands each variable defined, iterating through combinations for example (expand(join(out_dir,'{sp}_rulea.txt'),sp=sp_list)) expands to the following: expand(/path/to/out_dir/{sp}_rulea.txt) the sp_list is defined as ('sample_1' and 'sample_2'), the command expands further to: (/path/to/out_dir/sample_1_rulea.txt,/out/dir/sample_2_rulea.txt) Rule A \u00b6 Rule A requires that wildcards {sp} used in both the input and output. This wildcard is defined in the rule all, by setting the output of rule A with definition as sp=sp_list . # ruleA output expand(join(out_dir,'{sp}_rulea.txt'),sp=sp_list), An example of this rule's execution follows: rule A: input: fq = join(data_dir,'{sp}.fq') output: final = join(out_dir,'{sp}_rulea.txt') shell: ''' cat {input.fq} > {output.final} echo \"\\nruleA completed\" >> {output.final} ''' Rule B \u00b6 Rule B requires a function to be used in order to generate the required input. The function uses the wildcards feature to pull the information from the output {sp} . Again, this wildcard is defined in the rule all, by setting the output of rule B with sp=sp_list . An example of this def and the rule's execution follows: def get_input_files(wildcards): #example: {data_dir}/{sample_id.fq} fq = join(data_dir,fastq_dict[wildcards.sp]) return(fq) rule B: input: fq = get_input_files output: final = join(out_dir,'{sp}_ruleb.txt') shell: ''' cat {input.fq} > {output.final} echo \"\\nruleB completed\" >> {output.final} ''' Rule C \u00b6 Rule C requires a function to perform a command, which is directed through params: cmd . In this example the command is a simple copy, where the output has changed names. An example of this def and the rule's execution follows: def get_rulec_cmd(wildcards): cmd='cat ' sp_paths='' for sp in sp_list: # set source (ruleB) and destination files source = join(out_dir, sp + '_rulea.txt') # create command sp_paths = source + ' ' + sp_paths # add output path destination = join(out_dir, 'final_output','merged_rulea.txt') cmd = cmd + sp_paths + ' >> ' + destination return(cmd) rule C: input: rulea = expand(join(out_dir,'{sp}_rulea.txt'),sp=sp_list) params: cmd = get_rulec_cmd output: final = join(out_dir,'final_output','merged_rulea.txt') shell: ''' # create the final output file touch {output.final} # run the cat command {params.cmd} ''' Rule D \u00b6 Rule D required that the input of the rule be the linked to Rule B. This is accomplished using the rules.output format. Since individual files are included through the iteration of sp in the rules all. If the output file was a single file, all output files of rule B would be given as a single list. An example of this def and the rule's execution follows: def get_ruled_cmd(wildcards): cmd='' for sp in sp_list: # set source (ruleB) and destination files source = join(out_dir, wildcards.sp + '_ruleb.txt') destination = join(out_dir, 'final_output', wildcards.sp + '_copied_ruleb.txt') # cp the the files cmd = 'cp ' + source + ' ' + destination + '; ' + cmd return(cmd) rule D: input: ruleb = rules.B.output.final params: cmd = get_ruled_cmd output: final = join(out_dir, 'final_output', '{sp}_copied_ruleb.txt') shell: ''' # run the command {params.cmd} ''' Temp files \u00b6 To designate a temp file, simply include temp prior to the file name in output, as shown below: rule B: input: fq = get_input_files params: rname='rule_B', output: final = temp(join(out_dir,'{sp}_ruleb.txt')) shell: ''' cat {input.fq} > {output.final} echo \"\\nruleB completed\" >> {output.final} ''' Link rule name to log files \u00b6 This must be done in two steps. First, each rule must include a param with the rname variable, as shown below: rule B: input: fq = get_input_files params: rname='rule_B', output: final = temp(join(out_dir,'{sp}_ruleb.txt')) shell: ''' cat {input.fq} > {output.final} echo \"\\nruleB completed\" >> {output.final} ''' Then, the param variable can be used in the submission of the sbatch file. For example {params.rname} is used below: \"sbatch --gres {cluster.gres} --cpus-per-task {cluster.threads} \\ -p {cluster.partition} -t {cluster.time} --mem {cluster.mem} --job-name={params.rname} \\ --output=${output_dir}/log/{params.rname}{cluster.output} \\ --error=${output_dir}/log/{params.rname}{cluster.error}\" Initalization \u00b6 Initialization features ensure that the pipeline configuration files and documentation is stored with the pipeline. This includes creating output dirs, not included in snakemake and copiying configuration files that are utilized within the pipeline. Finally, sed commands can be used to replace variables with either command_line inputs or current_directory inputs, as shown below: # create directories if they do not exist if [[ ! -d $output_dir ]]; then mkdir $output_dir; fi dir_list=(config log) for pd in \"${dir_list[@]}\"; do if [[ ! -d $output_dir/$pd ]]; then mkdir -p $output_dir/$pd; fi; done # saving files, updating the configs with correct paths files_save=('config/snakemake_config.yaml' 'workflow/Snakefile' 'config/cluster_config.yaml') for f in ${files_save[@]}; do # set absolute path of file f=\"${PIPELINE_HOME}/$f\" # create an array of the absolute path, with the delimiter of \"/\" IFS='/' read -r -a strarr <<< \"$f\" # replace the variables PIPELINE_HOME and OUTPUT_dir in any files within the files_save array ($f) # save this output to the file name (strarr[-1]) in the output_dir/config location sed -e \"s/PIPELINE_HOME/${PIPELINE_HOME//\\//\\\\/}/g\" -e \"s/OUTPUT_DIR/${output_dir//\\//\\\\/}/g\" $f > \"${output_dir}/config/${strarr[-1]}\" done Cluster notes \u00b6 Update the cluster_config.yaml for a specific rule by adding that rule name, and including the parameter to change. For example, to change the time limit from 2 hours to 1 hour and threads from 4 to 2 for Rule E: E: time: 00-01:00:00 threads: 2 An example of the full cluster command needed to excute the pipeline is as follow: sbatch --job-name=\"snakemake_tutorial\" \\ --gres=lscratch:200 \\ --time=120:00:00 \\ --output=${output_dir}/log/%j_%x.out \\ --mail-type=BEGIN,END,FAIL \\ snakemake -s $output_dir/config/Snakefile \\ --configfile $output_dir/config/snakemake_config.yaml \\ --printshellcmds \\ --verbose \\ --rerun-incomplete \\ --latency-wait 120 \\ --use-envmodules \\ --cores 1 \\ --cluster-config ${output_dir}/config/cluster_config.yaml \\ -j 5 \\ --cluster \\ \"sbatch --gres {cluster.gres} --cpus-per-task {cluster.threads} \\ -p {cluster.partition} -t {cluster.time} --mem {cluster.mem} --job-name={params.rname} \\ --output=${output_dir}/log/{params.rname}{cluster.output} \\ --error=${output_dir}/log/{params.rname}{cluster.error}\" The breakdown of this is to create an sbatch job which acts as the master job, and controls all subsequent jobs. This master job submits the snakemake command, with all accompanying Snakefile and config files. Finally, the cluster command submits the criterion for all rule-related sbatch jobs that will be used as the pipeline runs Rule E \u00b6 Rule E required the addition of envmodules which reads from the added config parameter samtools , as well as the threads option. An example of this rule's execution follows: rule E: input: bamfile = join(data_dir,'{sp}.bam') envmodules: config['samtools'] params: rname='rule_E', threads: getthreads(\"E\") output: final = join(out_dir, 'final_output', '{sp}.sam') shell: ''' samtools view -H -@ {threads} {input.bamfile} > {output.final} '''","title":"3. Snakemake Completed Example"},{"location":"usage/example/#dry-run-expected-output","text":"The output of the dry-run for Rule A should look as follows: job count min threads max threads ----- ------- ------------- ------------- A 2 1 1 all 1 1 1 total 3 1 1 The output of the dry-run for Rules A-B should look as follows: job count min threads max threads ----- ------- ------------- ------------- A 2 1 1 B 2 1 1 all 1 1 1 total 5 1 1 The output of the dry-run for Rules A-C should look as follows: job count min threads max threads ----- ------- ------------- ------------- A 2 1 1 B 2 1 1 C 1 1 1 all 1 1 1 total 6 1 1 The output of the dry-run for Rules A-D should look as follows: job count min threads max threads ----- ------- ------------- ------------- A 2 1 1 B 2 1 1 C 1 1 1 D 2 1 1 all 1 1 1 total 8 1 1 The output of the dry-run for Rules A-E should look as follows: job count min threads max threads ----- ------- ------------- ------------- A 2 1 1 B 2 1 1 C 1 1 1 D 2 1 1 E 2 1 1 all 1 1 1 total 10 1 1","title":"Dry Run Expected Output"},{"location":"usage/example/#expected-output-files-structure","text":"The output of Rule A: \u251c\u2500\u2500 sample_1_rulea.txt \u251c\u2500\u2500 sample_2_rulea.txt The output of Rule B: \u251c\u2500\u2500 sample_1_rulea.txt \u251c\u2500\u2500 sample_1_ruleb.txt \u251c\u2500\u2500 sample_2_rulea.txt \u251c\u2500\u2500 sample_2_ruleb.txt The output of Rule C: \u251c\u2500\u2500 final_output \u2502 \u251c\u2500\u2500 merged_rulea.txt \u251c\u2500\u2500 sample_1_rulea.txt \u251c\u2500\u2500 sample_1_ruleb.txt \u251c\u2500\u2500 sample_2_rulea.txt \u2514\u2500\u2500 sample_2_ruleb.txt The output of Rule D: \u251c\u2500\u2500 final_output \u2502 \u251c\u2500\u2500 merged_rulea.txt \u2502 \u251c\u2500\u2500 sample_1_copied_ruleb.txt \u2502 \u251c\u2500\u2500 sample_2_copied_ruleb.txt \u251c\u2500\u2500 sample_1_rulea.txt \u251c\u2500\u2500 sample_1_ruleb.txt \u251c\u2500\u2500 sample_2_rulea.txt \u2514\u2500\u2500 sample_2_ruleb.txt The output of advanced tasks: final_output/ \u251c\u2500\u2500 merged_rulea.txt \u251c\u2500\u2500 sample_1_copied_ruleb.txt \u2514\u2500\u2500 sample_2_copied_ruleb.txt The output of Rule E: final_output/ \u251c\u2500\u2500 merged_rulea.txt \u251c\u2500\u2500 sample_1_copied_ruleb.txt \u251c\u2500\u2500 sample_1.sam \u251c\u2500\u2500 sample_2_copied_ruleb.txt \u2514\u2500\u2500 sample_2.sam","title":"Expected Output Files / Structure"},{"location":"usage/example/#explanations","text":"","title":"Explanations"},{"location":"usage/example/#general-notes","text":"The join and expand functions the join function is used to create and manage path structure by converting all \",\" to the expand expands each variable defined, iterating through combinations for example (expand(join(out_dir,'{sp}_rulea.txt'),sp=sp_list)) expands to the following: expand(/path/to/out_dir/{sp}_rulea.txt) the sp_list is defined as ('sample_1' and 'sample_2'), the command expands further to: (/path/to/out_dir/sample_1_rulea.txt,/out/dir/sample_2_rulea.txt)","title":"General Notes"},{"location":"usage/example/#rule-a","text":"Rule A requires that wildcards {sp} used in both the input and output. This wildcard is defined in the rule all, by setting the output of rule A with definition as sp=sp_list . # ruleA output expand(join(out_dir,'{sp}_rulea.txt'),sp=sp_list), An example of this rule's execution follows: rule A: input: fq = join(data_dir,'{sp}.fq') output: final = join(out_dir,'{sp}_rulea.txt') shell: ''' cat {input.fq} > {output.final} echo \"\\nruleA completed\" >> {output.final} '''","title":"Rule A"},{"location":"usage/example/#rule-b","text":"Rule B requires a function to be used in order to generate the required input. The function uses the wildcards feature to pull the information from the output {sp} . Again, this wildcard is defined in the rule all, by setting the output of rule B with sp=sp_list . An example of this def and the rule's execution follows: def get_input_files(wildcards): #example: {data_dir}/{sample_id.fq} fq = join(data_dir,fastq_dict[wildcards.sp]) return(fq) rule B: input: fq = get_input_files output: final = join(out_dir,'{sp}_ruleb.txt') shell: ''' cat {input.fq} > {output.final} echo \"\\nruleB completed\" >> {output.final} '''","title":"Rule B"},{"location":"usage/example/#rule-c","text":"Rule C requires a function to perform a command, which is directed through params: cmd . In this example the command is a simple copy, where the output has changed names. An example of this def and the rule's execution follows: def get_rulec_cmd(wildcards): cmd='cat ' sp_paths='' for sp in sp_list: # set source (ruleB) and destination files source = join(out_dir, sp + '_rulea.txt') # create command sp_paths = source + ' ' + sp_paths # add output path destination = join(out_dir, 'final_output','merged_rulea.txt') cmd = cmd + sp_paths + ' >> ' + destination return(cmd) rule C: input: rulea = expand(join(out_dir,'{sp}_rulea.txt'),sp=sp_list) params: cmd = get_rulec_cmd output: final = join(out_dir,'final_output','merged_rulea.txt') shell: ''' # create the final output file touch {output.final} # run the cat command {params.cmd} '''","title":"Rule C"},{"location":"usage/example/#rule-d","text":"Rule D required that the input of the rule be the linked to Rule B. This is accomplished using the rules.output format. Since individual files are included through the iteration of sp in the rules all. If the output file was a single file, all output files of rule B would be given as a single list. An example of this def and the rule's execution follows: def get_ruled_cmd(wildcards): cmd='' for sp in sp_list: # set source (ruleB) and destination files source = join(out_dir, wildcards.sp + '_ruleb.txt') destination = join(out_dir, 'final_output', wildcards.sp + '_copied_ruleb.txt') # cp the the files cmd = 'cp ' + source + ' ' + destination + '; ' + cmd return(cmd) rule D: input: ruleb = rules.B.output.final params: cmd = get_ruled_cmd output: final = join(out_dir, 'final_output', '{sp}_copied_ruleb.txt') shell: ''' # run the command {params.cmd} '''","title":"Rule D"},{"location":"usage/example/#temp-files","text":"To designate a temp file, simply include temp prior to the file name in output, as shown below: rule B: input: fq = get_input_files params: rname='rule_B', output: final = temp(join(out_dir,'{sp}_ruleb.txt')) shell: ''' cat {input.fq} > {output.final} echo \"\\nruleB completed\" >> {output.final} '''","title":"Temp files"},{"location":"usage/example/#link-rule-name-to-log-files","text":"This must be done in two steps. First, each rule must include a param with the rname variable, as shown below: rule B: input: fq = get_input_files params: rname='rule_B', output: final = temp(join(out_dir,'{sp}_ruleb.txt')) shell: ''' cat {input.fq} > {output.final} echo \"\\nruleB completed\" >> {output.final} ''' Then, the param variable can be used in the submission of the sbatch file. For example {params.rname} is used below: \"sbatch --gres {cluster.gres} --cpus-per-task {cluster.threads} \\ -p {cluster.partition} -t {cluster.time} --mem {cluster.mem} --job-name={params.rname} \\ --output=${output_dir}/log/{params.rname}{cluster.output} \\ --error=${output_dir}/log/{params.rname}{cluster.error}\"","title":"Link rule name to log files"},{"location":"usage/example/#initalization","text":"Initialization features ensure that the pipeline configuration files and documentation is stored with the pipeline. This includes creating output dirs, not included in snakemake and copiying configuration files that are utilized within the pipeline. Finally, sed commands can be used to replace variables with either command_line inputs or current_directory inputs, as shown below: # create directories if they do not exist if [[ ! -d $output_dir ]]; then mkdir $output_dir; fi dir_list=(config log) for pd in \"${dir_list[@]}\"; do if [[ ! -d $output_dir/$pd ]]; then mkdir -p $output_dir/$pd; fi; done # saving files, updating the configs with correct paths files_save=('config/snakemake_config.yaml' 'workflow/Snakefile' 'config/cluster_config.yaml') for f in ${files_save[@]}; do # set absolute path of file f=\"${PIPELINE_HOME}/$f\" # create an array of the absolute path, with the delimiter of \"/\" IFS='/' read -r -a strarr <<< \"$f\" # replace the variables PIPELINE_HOME and OUTPUT_dir in any files within the files_save array ($f) # save this output to the file name (strarr[-1]) in the output_dir/config location sed -e \"s/PIPELINE_HOME/${PIPELINE_HOME//\\//\\\\/}/g\" -e \"s/OUTPUT_DIR/${output_dir//\\//\\\\/}/g\" $f > \"${output_dir}/config/${strarr[-1]}\" done","title":"Initalization"},{"location":"usage/example/#cluster-notes","text":"Update the cluster_config.yaml for a specific rule by adding that rule name, and including the parameter to change. For example, to change the time limit from 2 hours to 1 hour and threads from 4 to 2 for Rule E: E: time: 00-01:00:00 threads: 2 An example of the full cluster command needed to excute the pipeline is as follow: sbatch --job-name=\"snakemake_tutorial\" \\ --gres=lscratch:200 \\ --time=120:00:00 \\ --output=${output_dir}/log/%j_%x.out \\ --mail-type=BEGIN,END,FAIL \\ snakemake -s $output_dir/config/Snakefile \\ --configfile $output_dir/config/snakemake_config.yaml \\ --printshellcmds \\ --verbose \\ --rerun-incomplete \\ --latency-wait 120 \\ --use-envmodules \\ --cores 1 \\ --cluster-config ${output_dir}/config/cluster_config.yaml \\ -j 5 \\ --cluster \\ \"sbatch --gres {cluster.gres} --cpus-per-task {cluster.threads} \\ -p {cluster.partition} -t {cluster.time} --mem {cluster.mem} --job-name={params.rname} \\ --output=${output_dir}/log/{params.rname}{cluster.output} \\ --error=${output_dir}/log/{params.rname}{cluster.error}\" The breakdown of this is to create an sbatch job which acts as the master job, and controls all subsequent jobs. This master job submits the snakemake command, with all accompanying Snakefile and config files. Finally, the cluster command submits the criterion for all rule-related sbatch jobs that will be used as the pipeline runs","title":"Cluster notes"},{"location":"usage/example/#rule-e","text":"Rule E required the addition of envmodules which reads from the added config parameter samtools , as well as the threads option. An example of this rule's execution follows: rule E: input: bamfile = join(data_dir,'{sp}.bam') envmodules: config['samtools'] params: rname='rule_E', threads: getthreads(\"E\") output: final = join(out_dir, 'final_output', '{sp}.sam') shell: ''' samtools view -H -@ {threads} {input.bamfile} > {output.final} '''","title":"Rule E"},{"location":"usage/getting-started/","text":"Prepare the environment \u00b6 Connect to Biowulf and load an interactive session # login ssh -Y $USER@biowulf.nih.gov # load interactive session srun -N 1 -n 1 --time=12:00:00 -p interactive --mem=8gb --cpus-per-task=4 --pty bash Clone the Repo \u00b6 Clone the github repo cd /path/to/working/dir git clone https://github.com/slsevilla/snakemake_tutorial.git cd snakemake_tutorial Completing the Activty \u00b6 Two diretories were created within the repo. The pipeline_todo directory should be used to complete the tutorial, editing all files as needed. The pipeline_example directory has the activity completed and should be used a reference for completing each of the tasks. NOTE: There are multiple ways to complete each rule, so review the expected outputs within the pipeline_example/output directory if your code varies.","title":"1. Getting Started"},{"location":"usage/getting-started/#prepare-the-environment","text":"Connect to Biowulf and load an interactive session # login ssh -Y $USER@biowulf.nih.gov # load interactive session srun -N 1 -n 1 --time=12:00:00 -p interactive --mem=8gb --cpus-per-task=4 --pty bash","title":"Prepare the environment"},{"location":"usage/getting-started/#clone-the-repo","text":"Clone the github repo cd /path/to/working/dir git clone https://github.com/slsevilla/snakemake_tutorial.git cd snakemake_tutorial","title":"Clone the Repo"},{"location":"usage/getting-started/#completing-the-activty","text":"Two diretories were created within the repo. The pipeline_todo directory should be used to complete the tutorial, editing all files as needed. The pipeline_example directory has the activity completed and should be used a reference for completing each of the tasks. NOTE: There are multiple ways to complete each rule, so review the expected outputs within the pipeline_example/output directory if your code varies.","title":"Completing the Activty"},{"location":"usage/task/","text":"Overview \u00b6 Learn some of the basics of Snakemake through the following tutorial. Create a script to run Snakemake Create variables to run a snakemake_config file Create rules for scenarios Use script to invoke Snakemake Manifest Files \u00b6 Manifest files have already been created in the /snakemake_tutorial/manifest directory. This includes: sample_manifest.csv sample_id,fq_name,bam_name sample_1,sample_1.fq,sample_1.bam sample_2,sample_2.fq,sample_2.bam Activity \u00b6 The task can be broken up into A. pre-processing, B. sample handling, C. rule creation, and D. Advanced Commands. All edits should be completed in the /snakemake_tutorial/pipeline_todo/ directory. A. Pre-Processing \u00b6 Create the output_dir , and a subdirectory log Create two different Snakemake commands, one for a dry run and one for a local run to the run_snakemake.sh . The commands should be dry or local . Include the path to the workflow/Snakefile, the config/snakemake_config.yaml in both commands Include flags --printshellcmds, --verbose, --rerun-incomplete in both commands Include flag --cores 1 for the local command B. Sample Handling \u00b6 Create the parameters in the config/snakemake_config.yaml 'sampleManifest' which gives the path of the sampleManifest 'out_dir' which gives the path to the output dir (must exist) 'data_dir' which gives the path to the data dir found under \"/snakemake_tutorial/data/\" Create the sample dictionaries and project lists from the manifest in the workflow/Snakefile `CreateSampleDicts` creates a dictionary matching sample_id to fq_file and a dictionary which matches sample_id to bam_file `CreateProjLists` creates a project lists `sp_list` which contains all sample_ids, `fq_list` which contains all fq_file names, and `bam_list` which contains all bam_file names C. Basic activities \u00b6 Complete each of the following tasks, in order. Be sure to perform dry runs and complete runs between each rule creation. The Hints section below provides guidance on each rule, while the Example page provides a detailed explanation of rule creation and features. General Tasks Create rule_all for each rule one at a time in the workflow/Snakefile . Create rule_all input for all fq input files, from the fq_list Rule A input files should be {sample_id}.fq output should be {sample_id}_rulea.txt and should be output to the out_dir shell command should add a line \"ruleA completed on a new line\" to the original file Rule B input files should be get_input_files . this definition will look up the name of the fq by taking in the sample_id as a wildcard, and using the samp_dict output should be {sample_id}_ruleb.txt and should be output to the out_dir shell command should add a line \"ruleB completed on a new line\" to the original file Rule C input files should be all of Rule A's output files params should be def get_rulec_cmd which iterates through all samples and creates a command cat {sample1}_rulea.txt {sample2}_rulea.txt >> {final_file} output should be merged_rulea.txt and should be output to the out_dir/final_output shell command should touch the {final_file} , then run the cmd parameter Rule D input files should be directly linked to Rule B's output files params should be def get_ruled_cmd which iterates through all samples and creates a command cp /output/path/{sample_id}_ruleb.txt /output/path/final_output/{sample_id)_copies_ruleb.txt; for each sample output should be {sample_id}_copied_ruleb.txt and should be output to the out_dir/final_output shell command should run the cmd parameter D. Advanced activities \u00b6 Add features to the workflow/Snakefile : Designate temp files flag rule A and rule B files so they are deleted after the pipeline completion Link rule names to log files all rules must have a param called rname where the rule name is identified uniquely Add initializtion features to the pipeline Add features to the run_snakemake.sh file to include: check if output_dir or output_dir/log are created; if not create them during invocation of the run_snakemake.sh file copy the config/snakemake_config.yaml, config/cluster_config.yaml to the output_dir; ensure snakemake runs use these files update all config files with the output_dir variable given from the command line and pipeline_dir variable based on the invocation location of the pipeline; Utilize cluster for rules Add features to the run_snakemake.sh file to include: update the copies cluster_config.yaml to change the time limit from 2 hours to 1 hour and threads from 4 to 2 for Rule E Add a new command to the run_snakemake.sh file: name the new command cluster . This command will include all of the previous flags of local . expand the cluster command with sbatch additional flags: --job-name=\"snakemake_tutorial\" --gres=lscratch:200 --time=120:00:00 --output=${output_dir}/log/%j_%x.out --mail-type=BEGIN,END,FAIL expand the cluster command further, with additional snakemake flags: --latency-wait 120 --use-envmodules -j 5 --cluster-config ${output_dir}/config/cluster_config.yml expand the cluster command further, with additional snakemake cluster flags: cluster \"sbatch --gres {cluster.gres} --cpus-per-task {cluster.threads} -p {cluster.partition} -t {cluster.time} --mem {cluster.mem} --job-name={params.rname} --output=${output_dir}/log/{params.rname}{cluster.output} --error=${output_dir}/log/{params.rname}{cluster.error}\" Rule E General Tasks - Create rule_all input for all bam input files, from the bam_list input files should be {sample_id}.fq envmodules should load the samtools version samtools/1.15.1 from the snakemake_config.yaml file threads should use def getthreads params should have rname set as a unique rule name output should be {sample_id}.sam and should be output to the out_dir/final_output shell command should use samtools to output the header to a sam file Hints \u00b6 Rule A and rule B are using the same input files, but only differ in how these files are being referenced. There are times when the sample_id of an input file will match, but other times (as when taking in a multiplexed ID when they will not be the same). Rule A handles cases where they match, rule B handles cases where they would not match. Rule B invokes a function to define the input files. Read more about this here . Rule C uses the expand feature for to gather all required input files. Read more about this here . Rule C and Rule D are outputting data to a directory that does not exist ( out_dir/final_output ). Snakemake will automatically create directories that don't exist, when they are listed as output files. Rule C should use the def definted to iterate through all the samples created in the sp_list. Rule D requires a \"link\" to Rule B's outptu through the use of the rules.RuleName.output.OutputName . Read more about this here . Advanced commands require use of the temp feature of snakemake. Read more about this here . Advanced commands require the use of the cluster feature of snakemake. Read more about this here . Cluster config file will follow the variable format from Biowulf for all sbatch parameters Rule E requires outputting the header samtools view -H of a file","title":"2. Snakemake Activity"},{"location":"usage/task/#overview","text":"Learn some of the basics of Snakemake through the following tutorial. Create a script to run Snakemake Create variables to run a snakemake_config file Create rules for scenarios Use script to invoke Snakemake","title":"Overview"},{"location":"usage/task/#manifest-files","text":"Manifest files have already been created in the /snakemake_tutorial/manifest directory. This includes: sample_manifest.csv sample_id,fq_name,bam_name sample_1,sample_1.fq,sample_1.bam sample_2,sample_2.fq,sample_2.bam","title":"Manifest Files"},{"location":"usage/task/#activity","text":"The task can be broken up into A. pre-processing, B. sample handling, C. rule creation, and D. Advanced Commands. All edits should be completed in the /snakemake_tutorial/pipeline_todo/ directory.","title":"Activity"},{"location":"usage/task/#a-pre-processing","text":"Create the output_dir , and a subdirectory log Create two different Snakemake commands, one for a dry run and one for a local run to the run_snakemake.sh . The commands should be dry or local . Include the path to the workflow/Snakefile, the config/snakemake_config.yaml in both commands Include flags --printshellcmds, --verbose, --rerun-incomplete in both commands Include flag --cores 1 for the local command","title":"A. Pre-Processing"},{"location":"usage/task/#b-sample-handling","text":"Create the parameters in the config/snakemake_config.yaml 'sampleManifest' which gives the path of the sampleManifest 'out_dir' which gives the path to the output dir (must exist) 'data_dir' which gives the path to the data dir found under \"/snakemake_tutorial/data/\" Create the sample dictionaries and project lists from the manifest in the workflow/Snakefile `CreateSampleDicts` creates a dictionary matching sample_id to fq_file and a dictionary which matches sample_id to bam_file `CreateProjLists` creates a project lists `sp_list` which contains all sample_ids, `fq_list` which contains all fq_file names, and `bam_list` which contains all bam_file names","title":"B. Sample Handling"},{"location":"usage/task/#c-basic-activities","text":"Complete each of the following tasks, in order. Be sure to perform dry runs and complete runs between each rule creation. The Hints section below provides guidance on each rule, while the Example page provides a detailed explanation of rule creation and features. General Tasks Create rule_all for each rule one at a time in the workflow/Snakefile . Create rule_all input for all fq input files, from the fq_list Rule A input files should be {sample_id}.fq output should be {sample_id}_rulea.txt and should be output to the out_dir shell command should add a line \"ruleA completed on a new line\" to the original file Rule B input files should be get_input_files . this definition will look up the name of the fq by taking in the sample_id as a wildcard, and using the samp_dict output should be {sample_id}_ruleb.txt and should be output to the out_dir shell command should add a line \"ruleB completed on a new line\" to the original file Rule C input files should be all of Rule A's output files params should be def get_rulec_cmd which iterates through all samples and creates a command cat {sample1}_rulea.txt {sample2}_rulea.txt >> {final_file} output should be merged_rulea.txt and should be output to the out_dir/final_output shell command should touch the {final_file} , then run the cmd parameter Rule D input files should be directly linked to Rule B's output files params should be def get_ruled_cmd which iterates through all samples and creates a command cp /output/path/{sample_id}_ruleb.txt /output/path/final_output/{sample_id)_copies_ruleb.txt; for each sample output should be {sample_id}_copied_ruleb.txt and should be output to the out_dir/final_output shell command should run the cmd parameter","title":"C. Basic activities"},{"location":"usage/task/#d-advanced-activities","text":"Add features to the workflow/Snakefile : Designate temp files flag rule A and rule B files so they are deleted after the pipeline completion Link rule names to log files all rules must have a param called rname where the rule name is identified uniquely Add initializtion features to the pipeline Add features to the run_snakemake.sh file to include: check if output_dir or output_dir/log are created; if not create them during invocation of the run_snakemake.sh file copy the config/snakemake_config.yaml, config/cluster_config.yaml to the output_dir; ensure snakemake runs use these files update all config files with the output_dir variable given from the command line and pipeline_dir variable based on the invocation location of the pipeline; Utilize cluster for rules Add features to the run_snakemake.sh file to include: update the copies cluster_config.yaml to change the time limit from 2 hours to 1 hour and threads from 4 to 2 for Rule E Add a new command to the run_snakemake.sh file: name the new command cluster . This command will include all of the previous flags of local . expand the cluster command with sbatch additional flags: --job-name=\"snakemake_tutorial\" --gres=lscratch:200 --time=120:00:00 --output=${output_dir}/log/%j_%x.out --mail-type=BEGIN,END,FAIL expand the cluster command further, with additional snakemake flags: --latency-wait 120 --use-envmodules -j 5 --cluster-config ${output_dir}/config/cluster_config.yml expand the cluster command further, with additional snakemake cluster flags: cluster \"sbatch --gres {cluster.gres} --cpus-per-task {cluster.threads} -p {cluster.partition} -t {cluster.time} --mem {cluster.mem} --job-name={params.rname} --output=${output_dir}/log/{params.rname}{cluster.output} --error=${output_dir}/log/{params.rname}{cluster.error}\" Rule E General Tasks - Create rule_all input for all bam input files, from the bam_list input files should be {sample_id}.fq envmodules should load the samtools version samtools/1.15.1 from the snakemake_config.yaml file threads should use def getthreads params should have rname set as a unique rule name output should be {sample_id}.sam and should be output to the out_dir/final_output shell command should use samtools to output the header to a sam file","title":"D. Advanced activities"},{"location":"usage/task/#hints","text":"Rule A and rule B are using the same input files, but only differ in how these files are being referenced. There are times when the sample_id of an input file will match, but other times (as when taking in a multiplexed ID when they will not be the same). Rule A handles cases where they match, rule B handles cases where they would not match. Rule B invokes a function to define the input files. Read more about this here . Rule C uses the expand feature for to gather all required input files. Read more about this here . Rule C and Rule D are outputting data to a directory that does not exist ( out_dir/final_output ). Snakemake will automatically create directories that don't exist, when they are listed as output files. Rule C should use the def definted to iterate through all the samples created in the sp_list. Rule D requires a \"link\" to Rule B's outptu through the use of the rules.RuleName.output.OutputName . Read more about this here . Advanced commands require use of the temp feature of snakemake. Read more about this here . Advanced commands require the use of the cluster feature of snakemake. Read more about this here . Cluster config file will follow the variable format from Biowulf for all sbatch parameters Rule E requires outputting the header samtools view -H of a file","title":"Hints"}]}